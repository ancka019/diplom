{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50142433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c56ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "import functools as func\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c935d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac17ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30490, 1941])\n",
      "torch.Size([30490, 1941])\n"
     ]
    }
   ],
   "source": [
    "data_prefix = \"M5/\"\n",
    "data_path = data_prefix + \"sales_train_evaluation.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "data1 = data.iloc[:,6:]\n",
    "arr = np.array(data1)\n",
    "tt = torch.tensor(arr)\n",
    "print(tt.shape)\n",
    "tt2=torch.tensor(data1.values)\n",
    "print(tt2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7917b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([i for i in map(lambda d: sum(d)/1941.0, data1.values)])\n",
    "m = np.array([i for i in map(lambda d: max(d), data1.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620ba694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>d_5</th>\n",
       "      <th>d_6</th>\n",
       "      <th>d_7</th>\n",
       "      <th>d_8</th>\n",
       "      <th>d_9</th>\n",
       "      <th>d_10</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30375</th>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>40</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30378</th>\n",
       "      <td>47</td>\n",
       "      <td>38</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30387</th>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>46</td>\n",
       "      <td>54</td>\n",
       "      <td>104</td>\n",
       "      <td>77</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30414</th>\n",
       "      <td>32</td>\n",
       "      <td>48</td>\n",
       "      <td>91</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30470</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 1941 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       d_1  d_2  d_3  d_4  d_5  d_6  d_7  d_8  d_9  d_10  ...  d_1932  d_1933  \\\n",
       "339      9   24    0    0    7    8    0    3    0     7  ...      22       4   \n",
       "362     14   25   67   27    0   74   13   32   15     4  ...      28       6   \n",
       "1695    10   15   13    5   11    5    7   21    6     6  ...       9      10   \n",
       "1826    10    3    4    0    0    0    1    4    2     0  ...      13      27   \n",
       "1846    15   27   18   29   23   20    2    2   44    14  ...      10      12   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...     ...     ...   \n",
       "30375   57   31   18   31   31   52   40   76   39    26  ...       4      41   \n",
       "30378   47   38   28   27   23   32   33   52   33    33  ...      23      23   \n",
       "30387   40   47   18   18   17   46   54  104   77     8  ...      23      32   \n",
       "30414   32   48   91   39   19    1   74   88   19     0  ...       9      24   \n",
       "30470    4   13    5    1    5    0    2    4    6     7  ...       0       0   \n",
       "\n",
       "       d_1934  d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "339         7      26       1       4       1       5      18      24  \n",
       "362         0       2       5       1      26       1      16      12  \n",
       "1695        2       5       6      15       6       4       6       2  \n",
       "1826        6       2      12      15       2       3       4       1  \n",
       "1846       24      27      12       9       7      13      12      16  \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "30375      27       8      18      23       7      12      17      24  \n",
       "30378      24      19      21      21      23      22      22      26  \n",
       "30387      25      15      10      11      17      20      37      30  \n",
       "30414      30       7       9      39       2      14       4      32  \n",
       "30470       0       0       0       0       0       0       0       0  \n",
       "\n",
       "[367 rows x 1941 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data1[s>10]\n",
    "m1 = m[s>10]\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17d995fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([367, 1941])\n",
      "torch.Size([1941, 367])\n"
     ]
    }
   ],
   "source": [
    "tt2=torch.tensor(data2.values)\n",
    "m2t = torch.unsqueeze(torch.tensor(m1), 1)\n",
    "#tt2=tt2/(m2t)\n",
    "print(tt2.shape)\n",
    "#print(m2t)\n",
    "tt2=tt2.transpose(0,1)\n",
    "print(tt2.shape)\n",
    "tta = tt2.numpy()\n",
    "np.savetxt(\"M5.txt\", tta, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f8ef52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 14, 10, ..., 40, 32,  4],\n",
       "       [24, 25, 15, ..., 47, 48, 13],\n",
       "       [ 0, 67, 13, ..., 18, 91,  5],\n",
       "       ...,\n",
       "       [ 5,  1,  4, ..., 20, 14,  0],\n",
       "       [18, 16,  6, ..., 37,  4,  0],\n",
       "       [24, 12,  2, ..., 30, 32,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39971d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args, data):\n",
    "        super(Model, self).__init__()\n",
    "        self.use_cuda = args.cuda\n",
    "        self.P = args.window;\n",
    "        self.m = data.m\n",
    "        self.hidR = args.hidRNN;\n",
    "        self.hidC = args.hidCNN;\n",
    "        self.hidS = args.hidSkip;\n",
    "        self.Ck = args.CNN_kernel;\n",
    "        self.skip = args.skip;\n",
    "        self.pt = int((self.P - self.Ck)/self.skip)\n",
    "        self.hw = args.highway_window\n",
    "        self.conv1 = nn.Conv2d(1, self.hidC, kernel_size = (self.Ck, self.m));\n",
    "        self.GRU1 = nn.GRU(self.hidC, self.hidR);\n",
    "        self.dropout = nn.Dropout(p = args.dropout);\n",
    "        if (self.skip > 0):\n",
    "            self.GRUskip = nn.GRU(self.hidC, self.hidS);\n",
    "            self.linear1 = nn.Linear(self.hidR + self.skip * self.hidS, self.m);\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(self.hidR, self.m);\n",
    "        if (self.hw > 0):\n",
    "            self.highway = nn.Linear(self.hw, 1);\n",
    "        self.output = None;\n",
    "        if (args.output_fun == 'sigmoid'):\n",
    "            self.output = F.sigmoid;\n",
    "        if (args.output_fun == 'tanh'):\n",
    "            self.output = F.tanh;\n",
    " \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0);\n",
    "        \n",
    "        #CNN\n",
    "        c = x.view(-1, 1, self.P, self.m);\n",
    "        c = F.relu(self.conv1(c));\n",
    "        c = self.dropout(c);\n",
    "        c = torch.squeeze(c, 3);\n",
    "        \n",
    "        # RNN \n",
    "        r = c.permute(2, 0, 1).contiguous();\n",
    "        _, r = self.GRU1(r);\n",
    "        r = self.dropout(torch.squeeze(r,0));\n",
    "\n",
    "        \n",
    "        #skip-rnn\n",
    "        \n",
    "        if (self.skip > 0):\n",
    "            s = c[:,:, int(-self.pt * self.skip):].contiguous();\n",
    "            s = s.view(batch_size, self.hidC, self.pt, self.skip);\n",
    "            s = s.permute(2,0,3,1).contiguous();\n",
    "            s = s.view(self.pt, batch_size * self.skip, self.hidC);\n",
    "            _, s = self.GRUskip(s);\n",
    "            s = s.view(batch_size, self.skip * self.hidS);\n",
    "            s = self.dropout(s);\n",
    "            r = torch.cat((r,s),1);\n",
    "        \n",
    "        res = self.linear1(r);\n",
    "        \n",
    "        #highway\n",
    "        if (self.hw > 0):\n",
    "            z = x[:, -self.hw:, :];\n",
    "            z = z.permute(0,2,1).contiguous().view(-1, self.hw);\n",
    "            z = self.highway(z);\n",
    "            z = z.view(-1,self.m);\n",
    "            res = res + z;\n",
    "            \n",
    "        if (self.output):\n",
    "            res = self.output(res);\n",
    "        return res;\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a05ea855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.optim\n",
    "\n",
    "class Optim(object):\n",
    "\n",
    "    def _makeOptimizer(self):\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = torch.optim.SGD(self.params, lr=self.lr)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = torch.optim.Adagrad(self.params, lr=self.lr)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = torch.optim.Adadelta(self.params, lr=self.lr)\n",
    "        elif self.method == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(self.params, lr=self.lr)\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "    def __init__(self, params, method, lr, max_grad_norm, lr_decay=1, start_decay_at=None):\n",
    "        self.params = list(params)  # careful: params may be a generator\n",
    "        self.last_ppl = None\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_at = start_decay_at\n",
    "        self.start_decay = False\n",
    "\n",
    "        self._makeOptimizer()\n",
    "\n",
    "    def step(self):\n",
    "        # Compute gradients norm.\n",
    "        grad_norm = 0\n",
    "        for param in self.params:\n",
    "            grad_norm += math.pow(param.grad.data.norm(), 2)\n",
    "\n",
    "        grad_norm = math.sqrt(grad_norm)\n",
    "        if grad_norm > 0:\n",
    "            shrinkage = self.max_grad_norm / grad_norm\n",
    "        else:\n",
    "            shrinkage = 1.\n",
    "\n",
    "        for param in self.params:\n",
    "            if shrinkage < 1:\n",
    "                param.grad.data.mul_(shrinkage)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        return grad_norm\n",
    "\n",
    "    # decay learning rate if val perf does not improve or we hit the start_decay_at limit\n",
    "    def updateLearningRate(self, ppl, epoch):\n",
    "        if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
    "            self.start_decay = True\n",
    "        if self.last_ppl is not None and ppl > self.last_ppl:\n",
    "            self.start_decay = True\n",
    "\n",
    "        if self.start_decay:\n",
    "            self.lr = self.lr * self.lr_decay\n",
    "            print(\"Decaying learning rate to %g\" % self.lr)\n",
    "        #only decay for one epoch\n",
    "        self.start_decay = False\n",
    "\n",
    "        self.last_ppl = ppl\n",
    "\n",
    "        self._makeOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e810ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41415ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca36a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6edbf760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np;\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def normal_std(x):\n",
    "    return x.std() * np.sqrt((len(x) - 1.)/(len(x)))\n",
    "\n",
    "class Data_utility(object):\n",
    "    # train and valid is the ratio of training set and validation set. test = 1 - train - valid\n",
    "    def __init__(self, file_name, train, valid, cuda, horizon, window, normalize = 2, def_scale = 1):\n",
    "        self.cuda = cuda;\n",
    "        self.P = window;\n",
    "        self.h = horizon\n",
    "        fin = open(file_name);\n",
    "        self.rawdat = np.loadtxt(fin,delimiter=',');\n",
    "        self.dat = np.zeros(self.rawdat.shape);\n",
    "        self.n, self.m = self.dat.shape;\n",
    "        self.normalize = 2\n",
    "        self.scale = np.ones(self.m)*def_scale;\n",
    "        self._normalized(normalize);\n",
    "        self._split(int(train * self.n), int((train+valid) * self.n), self.n);\n",
    "        \n",
    "        self.scale = torch.from_numpy(self.scale).float();\n",
    "        tmp = self.test[1] * self.scale.expand(self.test[1].size(0), self.m);\n",
    "            \n",
    "        if self.cuda:\n",
    "            self.scale = self.scale.cuda();\n",
    "        self.scale = Variable(self.scale);\n",
    "        \n",
    "        self.rse = normal_std(tmp);\n",
    "        self.rae = torch.mean(torch.abs(tmp - torch.mean(tmp)));\n",
    "    \n",
    "    def _normalized(self, normalize):\n",
    "        #normalized by the maximum value of entire matrix.\n",
    "       \n",
    "        if (normalize == 0):\n",
    "            self.dat = self.rawdat\n",
    "            \n",
    "        if (normalize == 1):\n",
    "            self.dat = self.rawdat / np.max(self.rawdat);\n",
    "            \n",
    "        #normlized by the maximum value of each row(sensor).\n",
    "        if (normalize == 2):\n",
    "            for i in range(self.m):\n",
    "                self.scale[i] = np.max(np.abs(self.rawdat[:,i]));\n",
    "                self.dat[:,i] = self.rawdat[:,i] / np.max(np.abs(self.rawdat[:,i]));\n",
    "            \n",
    "        \n",
    "    def _split(self, train, valid, test):\n",
    "        \n",
    "        train_set = range(self.P+self.h-1, train);\n",
    "        valid_set = range(train, valid);\n",
    "        test_set = range(valid, self.n);\n",
    "        self.train = self._batchify(train_set, self.h);\n",
    "        self.valid = self._batchify(valid_set, self.h);\n",
    "        self.test = self._batchify(test_set, self.h);\n",
    "        \n",
    "        \n",
    "    def _batchify(self, idx_set, horizon):\n",
    "        \n",
    "        n = len(idx_set);\n",
    "        X = torch.zeros((n,self.P,self.m));\n",
    "        Y = torch.zeros((n,self.m));\n",
    "        \n",
    "        for i in range(n):\n",
    "            end = idx_set[i] - self.h + 1;\n",
    "            start = end - self.P;\n",
    "            X[i,:,:] = torch.from_numpy(self.dat[start:end, :]);\n",
    "            Y[i,:] = torch.from_numpy(self.dat[idx_set[i], :]);\n",
    "\n",
    "        return [X, Y];\n",
    "\n",
    "    def get_batches(self, inputs, targets, batch_size, shuffle=True):\n",
    "        length = len(inputs)\n",
    "        if shuffle:\n",
    "            index = torch.randperm(length)\n",
    "        else:\n",
    "            index = torch.LongTensor(range(length))\n",
    "        start_idx = 0\n",
    "        while (start_idx < length):\n",
    "            end_idx = min(length, start_idx + batch_size)\n",
    "            excerpt = index[start_idx:end_idx]\n",
    "            X = inputs[excerpt]; Y = targets[excerpt];\n",
    "            if (self.cuda):\n",
    "                X = X.cuda();\n",
    "                Y = Y.cuda();  \n",
    "            yield Variable(X), Variable(Y);\n",
    "            start_idx += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22d4f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np;\n",
    "import importlib\n",
    "\n",
    "\n",
    "def evaluate(data, X, Y, model, evaluateL2, evaluateL1, batch_size):\n",
    "    model.eval();\n",
    "    total_loss = 0;\n",
    "    total_loss_l1 = 0;\n",
    "    n_samples = 0;\n",
    "    predict = None;\n",
    "    test = None;\n",
    "    \n",
    "    for X, Y in data.get_batches(X, Y, batch_size, False):\n",
    "        output = model(X);\n",
    "        if predict is None:\n",
    "            predict = output;\n",
    "            test = Y;\n",
    "        else:\n",
    "            predict = torch.cat((predict,output));\n",
    "            test = torch.cat((test, Y));\n",
    "        \n",
    "        scale = data.scale.expand(output.size(0), data.m)\n",
    "        total_loss += evaluateL2(output * scale, Y * scale).item()\n",
    "        total_loss_l1 += evaluateL1(output * scale, Y * scale).item()\n",
    "        n_samples += (output.size(0) * data.m);\n",
    "    rse = math.sqrt(total_loss / n_samples)/data.rse\n",
    "    rae = (total_loss_l1/n_samples)/data.rae\n",
    "    \n",
    "    predict = predict.data.cpu().numpy();\n",
    "    Ytest = test.data.cpu().numpy();\n",
    "    sigma_p = (predict).std(axis = 0);\n",
    "    sigma_g = (Ytest).std(axis = 0);\n",
    "    mean_p = predict.mean(axis = 0)\n",
    "    mean_g = Ytest.mean(axis = 0)\n",
    "    index = (sigma_g!=0);\n",
    "    correlation = ((predict - mean_p) * (Ytest - mean_g)).mean(axis = 0)/(sigma_p * sigma_g);\n",
    "    correlation = (correlation[index]).mean();\n",
    "    return rse, rae, correlation, predict, Ytest;\n",
    "\n",
    "def train(data, X, Y, model, criterion, optim, batch_size):\n",
    "    model.train();\n",
    "    total_loss = 0;\n",
    "    n_samples = 0;\n",
    "    for X, Y in data.get_batches(X, Y, batch_size, True):\n",
    "        model.zero_grad();\n",
    "        output = model(X);\n",
    "        scale = data.scale.expand(output.size(0), data.m)\n",
    "        loss = criterion(output * scale, Y * scale);\n",
    "        loss.backward();\n",
    "        grad_norm = optim.step();\n",
    "        total_loss += loss.item();\n",
    "        n_samples += (output.size(0) * data.m);\n",
    "    return total_loss / n_samples\n",
    "\n",
    "sys.argv = \"main --gpu 0 --horizon 24 --data data/electricity.txt --save save/elec.pt --output_fun Linear\".split(\" \")\n",
    "    \n",
    "parser = argparse.ArgumentParser(description='PyTorch Time series forecasting')\n",
    "parser.add_argument('--data', type=str, required=True,\n",
    "                    help='location of the data file')\n",
    "parser.add_argument('--model', type=str, default='LSTNet',\n",
    "                    help='')\n",
    "parser.add_argument('--hidCNN', type=int, default=100,\n",
    "                    help='number of CNN hidden units')\n",
    "parser.add_argument('--hidRNN', type=int, default=100,\n",
    "                    help='number of RNN hidden units')\n",
    "parser.add_argument('--window', type=int, default=24 * 7,\n",
    "                    help='window size')\n",
    "parser.add_argument('--CNN_kernel', type=int, default=6,\n",
    "                    help='the kernel size of the CNN layers')\n",
    "parser.add_argument('--highway_window', type=int, default=24,\n",
    "                    help='The window size of the highway component')\n",
    "parser.add_argument('--clip', type=float, default=10.,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=128, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--seed', type=int, default=54321,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--log_interval', type=int, default=2000, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str,  default='model/model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--cuda', type=str, default=True)\n",
    "parser.add_argument('--optim', type=str, default='adam')\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--horizon', type=int, default=12)\n",
    "parser.add_argument('--skip', type=float, default=24)\n",
    "parser.add_argument('--hidSkip', type=int, default=5)\n",
    "parser.add_argument('--L1Loss', type=bool, default=True)\n",
    "parser.add_argument('--normalize', type=int, default=2)\n",
    "parser.add_argument('--output_fun', type=str, default='sigmoid')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.data = 'M5.txt'\n",
    "args.save = 'M5.pt'\n",
    "args.normalize = 0\n",
    "args.epochs = 400\n",
    "args.skip = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a429ad9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9.000000000000000000e+00</th>\n",
       "      <th>1.400000000000000000e+01</th>\n",
       "      <th>1.000000000000000000e+01</th>\n",
       "      <th>1.000000000000000000e+01.1</th>\n",
       "      <th>1.500000000000000000e+01</th>\n",
       "      <th>3.800000000000000000e+01</th>\n",
       "      <th>3.200000000000000000e+01</th>\n",
       "      <th>2.100000000000000000e+01</th>\n",
       "      <th>1.300000000000000000e+01</th>\n",
       "      <th>3.000000000000000000e+01</th>\n",
       "      <th>...</th>\n",
       "      <th>5.700000000000000000e+01</th>\n",
       "      <th>5.000000000000000000e+00.10</th>\n",
       "      <th>1.200000000000000000e+01.7</th>\n",
       "      <th>2.800000000000000000e+01.3</th>\n",
       "      <th>4.700000000000000000e+01.2</th>\n",
       "      <th>5.700000000000000000e+01.1</th>\n",
       "      <th>4.700000000000000000e+01.3</th>\n",
       "      <th>4.000000000000000000e+01</th>\n",
       "      <th>3.200000000000000000e+01.2</th>\n",
       "      <th>4.000000000000000000e+00.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      9.000000000000000000e+00  1.400000000000000000e+01  \\\n",
       "0                         24.0                      25.0   \n",
       "1                          0.0                      67.0   \n",
       "2                          0.0                      27.0   \n",
       "3                          7.0                       0.0   \n",
       "4                          8.0                      74.0   \n",
       "...                        ...                       ...   \n",
       "1935                       4.0                       1.0   \n",
       "1936                       1.0                      26.0   \n",
       "1937                       5.0                       1.0   \n",
       "1938                      18.0                      16.0   \n",
       "1939                      24.0                      12.0   \n",
       "\n",
       "      1.000000000000000000e+01  1.000000000000000000e+01.1  \\\n",
       "0                         15.0                         3.0   \n",
       "1                         13.0                         4.0   \n",
       "2                          5.0                         0.0   \n",
       "3                         11.0                         0.0   \n",
       "4                          5.0                         0.0   \n",
       "...                        ...                         ...   \n",
       "1935                      15.0                        15.0   \n",
       "1936                       6.0                         2.0   \n",
       "1937                       4.0                         3.0   \n",
       "1938                       6.0                         4.0   \n",
       "1939                       2.0                         1.0   \n",
       "\n",
       "      1.500000000000000000e+01  3.800000000000000000e+01  \\\n",
       "0                         27.0                      31.0   \n",
       "1                         18.0                      14.0   \n",
       "2                         29.0                      29.0   \n",
       "3                         23.0                      24.0   \n",
       "4                         20.0                      43.0   \n",
       "...                        ...                       ...   \n",
       "1935                       9.0                      12.0   \n",
       "1936                       7.0                      21.0   \n",
       "1937                      13.0                      16.0   \n",
       "1938                      12.0                      29.0   \n",
       "1939                      16.0                      26.0   \n",
       "\n",
       "      3.200000000000000000e+01  2.100000000000000000e+01  \\\n",
       "0                         24.0                      37.0   \n",
       "1                         12.0                      24.0   \n",
       "2                         21.0                      32.0   \n",
       "3                         14.0                      16.0   \n",
       "4                         20.0                      31.0   \n",
       "...                        ...                       ...   \n",
       "1935                       7.0                       3.0   \n",
       "1936                       6.0                       7.0   \n",
       "1937                       4.0                       5.0   \n",
       "1938                       9.0                       5.0   \n",
       "1939                      10.0                      19.0   \n",
       "\n",
       "      1.300000000000000000e+01  3.000000000000000000e+01  ...  \\\n",
       "0                          1.0                      45.0  ...   \n",
       "1                          0.0                      14.0  ...   \n",
       "2                         17.0                      52.0  ...   \n",
       "3                          7.0                      27.0  ...   \n",
       "4                          0.0                      29.0  ...   \n",
       "...                        ...                       ...  ...   \n",
       "1935                      10.0                      12.0  ...   \n",
       "1936                       9.0                      11.0  ...   \n",
       "1937                       8.0                       7.0  ...   \n",
       "1938                      11.0                      11.0  ...   \n",
       "1939                       9.0                      14.0  ...   \n",
       "\n",
       "      5.700000000000000000e+01  5.000000000000000000e+00.10  \\\n",
       "0                         62.0                          6.0   \n",
       "1                         49.0                          8.0   \n",
       "2                         36.0                          2.0   \n",
       "3                          0.0                          2.0   \n",
       "4                          0.0                          7.0   \n",
       "...                        ...                          ...   \n",
       "1935                      20.0                          0.0   \n",
       "1936                      19.0                          0.0   \n",
       "1937                      25.0                          0.0   \n",
       "1938                      17.0                          0.0   \n",
       "1939                      24.0                          0.0   \n",
       "\n",
       "      1.200000000000000000e+01.7  2.800000000000000000e+01.3  \\\n",
       "0                           10.0                        25.0   \n",
       "1                           14.0                        14.0   \n",
       "2                           13.0                        17.0   \n",
       "3                           11.0                        14.0   \n",
       "4                            9.0                        25.0   \n",
       "...                          ...                         ...   \n",
       "1935                        13.0                        26.0   \n",
       "1936                        12.0                        34.0   \n",
       "1937                         9.0                        48.0   \n",
       "1938                        12.0                        82.0   \n",
       "1939                        16.0                        64.0   \n",
       "\n",
       "      4.700000000000000000e+01.2  5.700000000000000000e+01.1  \\\n",
       "0                           40.0                        31.0   \n",
       "1                           48.0                        18.0   \n",
       "2                           54.0                        31.0   \n",
       "3                           31.0                        31.0   \n",
       "4                           58.0                        52.0   \n",
       "...                          ...                         ...   \n",
       "1935                        30.0                        23.0   \n",
       "1936                        45.0                         7.0   \n",
       "1937                        51.0                        12.0   \n",
       "1938                        46.0                        17.0   \n",
       "1939                        48.0                        24.0   \n",
       "\n",
       "      4.700000000000000000e+01.3  4.000000000000000000e+01  \\\n",
       "0                           38.0                      47.0   \n",
       "1                           28.0                      18.0   \n",
       "2                           27.0                      18.0   \n",
       "3                           23.0                      17.0   \n",
       "4                           32.0                      46.0   \n",
       "...                          ...                       ...   \n",
       "1935                        21.0                      11.0   \n",
       "1936                        23.0                      17.0   \n",
       "1937                        22.0                      20.0   \n",
       "1938                        22.0                      37.0   \n",
       "1939                        26.0                      30.0   \n",
       "\n",
       "      3.200000000000000000e+01.2  4.000000000000000000e+00.5  \n",
       "0                           48.0                        13.0  \n",
       "1                           91.0                         5.0  \n",
       "2                           39.0                         1.0  \n",
       "3                           19.0                         5.0  \n",
       "4                            1.0                         0.0  \n",
       "...                          ...                         ...  \n",
       "1935                        39.0                         0.0  \n",
       "1936                         2.0                         0.0  \n",
       "1937                        14.0                         0.0  \n",
       "1938                         4.0                         0.0  \n",
       "1939                        32.0                         0.0  \n",
       "\n",
       "[1940 rows x 367 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argdata = pd.read_csv(args.data)\n",
    "argdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca122985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.5151)\n",
      "* number of parameters: 332442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args.cuda = args.gpu is not None\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "Data = Data_utility(args.data, 0.6, 0.2, args.cuda, args.horizon, args.window, args.normalize, def_scale=1);\n",
    "print(Data.rse);\n",
    "\n",
    "model = Model(args, Data);\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('* number of parameters: %d' % nParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2271db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "if args.L1Loss:\n",
    "    criterion = nn.L1Loss(size_average=False);\n",
    "else:\n",
    "    criterion = nn.MSELoss(size_average=False);\n",
    "evaluateL2 = nn.MSELoss(size_average=False);\n",
    "evaluateL1 = nn.L1Loss(size_average=False)\n",
    "if args.cuda:\n",
    "    criterion = criterion.cuda()\n",
    "    evaluateL1 = evaluateL1.cuda();\n",
    "    evaluateL2 = evaluateL2.cuda();\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e9aeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "best_val = 10000000;\n",
    "optim = Optim(\n",
    "    model.parameters(), args.optim, args.lr, args.clip,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "825c99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "| end of epoch   1 | time:  0.23s | train_loss 28.3843 | valid rse 1.7381 | valid rae 1.8388 | valid corr  -0.0047\n",
      "| end of epoch   2 | time:  0.21s | train_loss 23.9161 | valid rse 1.5088 | valid rae 1.5413 | valid corr  0.0302\n",
      "| end of epoch   3 | time:  0.19s | train_loss 19.6918 | valid rse 1.2971 | valid rae 1.2791 | valid corr  0.0723\n",
      "| end of epoch   4 | time:  0.20s | train_loss 15.9519 | valid rse 1.1072 | valid rae 1.0547 | valid corr  0.1051\n",
      "| end of epoch   5 | time:  0.20s | train_loss 12.9826 | valid rse 0.9565 | valid rae 0.8875 | valid corr  0.1235\n",
      "test rse 0.9239 | test rae 0.8517 | test corr 0.1323\n",
      "| end of epoch   6 | time:  0.19s | train_loss 11.1244 | valid rse 0.8720 | valid rae 0.8078 | valid corr  0.1327\n",
      "| end of epoch   7 | time:  0.18s | train_loss 10.5209 | valid rse 0.8533 | valid rae 0.7928 | valid corr  0.1386\n",
      "| end of epoch   8 | time:  0.19s | train_loss 10.2699 | valid rse 0.8543 | valid rae 0.7849 | valid corr  0.1432\n",
      "| end of epoch   9 | time:  0.24s | train_loss 10.1183 | valid rse 0.8476 | valid rae 0.7766 | valid corr  0.1499\n",
      "| end of epoch  10 | time:  0.18s | train_loss 10.0067 | valid rse 0.8422 | valid rae 0.7701 | valid corr  0.1557\n",
      "test rse 0.7973 | test rae 0.7199 | test corr 0.1713\n",
      "| end of epoch  11 | time:  0.18s | train_loss 9.9159 | valid rse 0.8382 | valid rae 0.7643 | valid corr  0.1629\n",
      "| end of epoch  12 | time:  0.18s | train_loss 9.8321 | valid rse 0.8317 | valid rae 0.7580 | valid corr  0.1694\n",
      "| end of epoch  13 | time:  0.18s | train_loss 9.7578 | valid rse 0.8281 | valid rae 0.7535 | valid corr  0.1762\n",
      "| end of epoch  14 | time:  0.20s | train_loss 9.6901 | valid rse 0.8236 | valid rae 0.7485 | valid corr  0.1829\n",
      "| end of epoch  15 | time:  0.25s | train_loss 9.6223 | valid rse 0.8204 | valid rae 0.7448 | valid corr  0.1893\n",
      "test rse 0.7734 | test rae 0.6933 | test corr 0.2097\n",
      "| end of epoch  16 | time:  0.18s | train_loss 9.5674 | valid rse 0.8172 | valid rae 0.7414 | valid corr  0.1937\n",
      "| end of epoch  17 | time:  0.18s | train_loss 9.5222 | valid rse 0.8151 | valid rae 0.7385 | valid corr  0.1993\n",
      "| end of epoch  18 | time:  0.20s | train_loss 9.4794 | valid rse 0.8120 | valid rae 0.7355 | valid corr  0.2034\n",
      "| end of epoch  19 | time:  0.20s | train_loss 9.4468 | valid rse 0.8110 | valid rae 0.7336 | valid corr  0.2068\n",
      "| end of epoch  20 | time:  0.22s | train_loss 9.4176 | valid rse 0.8080 | valid rae 0.7311 | valid corr  0.2097\n",
      "test rse 0.7592 | test rae 0.6791 | test corr 0.2343\n",
      "| end of epoch  21 | time:  0.25s | train_loss 9.3899 | valid rse 0.8070 | valid rae 0.7291 | valid corr  0.2128\n",
      "| end of epoch  22 | time:  0.19s | train_loss 9.3657 | valid rse 0.8051 | valid rae 0.7268 | valid corr  0.2168\n",
      "| end of epoch  23 | time:  0.18s | train_loss 9.3448 | valid rse 0.8049 | valid rae 0.7265 | valid corr  0.2177\n",
      "| end of epoch  24 | time:  0.19s | train_loss 9.3266 | valid rse 0.8041 | valid rae 0.7251 | valid corr  0.2192\n",
      "| end of epoch  25 | time:  0.19s | train_loss 9.3128 | valid rse 0.8059 | valid rae 0.7262 | valid corr  0.2200\n",
      "test rse 0.7564 | test rae 0.6729 | test corr 0.2469\n",
      "| end of epoch  26 | time:  0.22s | train_loss 9.3062 | valid rse 0.8048 | valid rae 0.7252 | valid corr  0.2223\n",
      "| end of epoch  27 | time:  0.26s | train_loss 9.3073 | valid rse 0.8028 | valid rae 0.7240 | valid corr  0.2224\n",
      "| end of epoch  28 | time:  0.19s | train_loss 9.2941 | valid rse 0.8034 | valid rae 0.7244 | valid corr  0.2233\n",
      "| end of epoch  29 | time:  0.19s | train_loss 9.2970 | valid rse 0.8026 | valid rae 0.7235 | valid corr  0.2234\n",
      "| end of epoch  30 | time:  0.18s | train_loss 9.2837 | valid rse 0.8023 | valid rae 0.7233 | valid corr  0.2236\n",
      "test rse 0.7522 | test rae 0.6701 | test corr 0.2511\n",
      "| end of epoch  31 | time:  0.18s | train_loss 9.2763 | valid rse 0.8040 | valid rae 0.7235 | valid corr  0.2242\n",
      "| end of epoch  32 | time:  0.25s | train_loss 9.2697 | valid rse 0.8023 | valid rae 0.7223 | valid corr  0.2238\n",
      "| end of epoch  33 | time:  0.18s | train_loss 9.2618 | valid rse 0.8015 | valid rae 0.7215 | valid corr  0.2241\n",
      "| end of epoch  34 | time:  0.25s | train_loss 9.2521 | valid rse 0.8023 | valid rae 0.7218 | valid corr  0.2260\n",
      "| end of epoch  35 | time:  0.18s | train_loss 9.2443 | valid rse 0.8023 | valid rae 0.7219 | valid corr  0.2269\n",
      "test rse 0.7532 | test rae 0.6707 | test corr 0.2551\n",
      "| end of epoch  36 | time:  0.18s | train_loss 9.2393 | valid rse 0.8003 | valid rae 0.7206 | valid corr  0.2258\n",
      "| end of epoch  37 | time:  0.18s | train_loss 9.2331 | valid rse 0.8009 | valid rae 0.7208 | valid corr  0.2275\n",
      "| end of epoch  38 | time:  0.18s | train_loss 9.2235 | valid rse 0.7989 | valid rae 0.7199 | valid corr  0.2273\n",
      "| end of epoch  39 | time:  0.23s | train_loss 9.2279 | valid rse 0.8020 | valid rae 0.7220 | valid corr  0.2295\n",
      "| end of epoch  40 | time:  0.18s | train_loss 9.2227 | valid rse 0.7992 | valid rae 0.7198 | valid corr  0.2280\n",
      "test rse 0.7497 | test rae 0.6674 | test corr 0.2565\n",
      "| end of epoch  41 | time:  0.23s | train_loss 9.2101 | valid rse 0.7995 | valid rae 0.7199 | valid corr  0.2293\n",
      "| end of epoch  42 | time:  0.18s | train_loss 9.1980 | valid rse 0.8001 | valid rae 0.7208 | valid corr  0.2271\n",
      "| end of epoch  43 | time:  0.18s | train_loss 9.2034 | valid rse 0.8006 | valid rae 0.7208 | valid corr  0.2303\n",
      "| end of epoch  44 | time:  0.18s | train_loss 9.2058 | valid rse 0.7992 | valid rae 0.7194 | valid corr  0.2289\n",
      "| end of epoch  45 | time:  0.21s | train_loss 9.1998 | valid rse 0.8017 | valid rae 0.7218 | valid corr  0.2296\n",
      "test rse 0.7539 | test rae 0.6712 | test corr 0.2580\n",
      "| end of epoch  46 | time:  0.18s | train_loss 9.1911 | valid rse 0.8008 | valid rae 0.7211 | valid corr  0.2297\n",
      "| end of epoch  47 | time:  0.23s | train_loss 9.1883 | valid rse 0.8055 | valid rae 0.7256 | valid corr  0.2313\n",
      "| end of epoch  48 | time:  0.18s | train_loss 9.1791 | valid rse 0.7985 | valid rae 0.7198 | valid corr  0.2284\n",
      "| end of epoch  49 | time:  0.18s | train_loss 9.1725 | valid rse 0.8015 | valid rae 0.7218 | valid corr  0.2296\n",
      "| end of epoch  50 | time:  0.18s | train_loss 9.1715 | valid rse 0.8006 | valid rae 0.7213 | valid corr  0.2289\n",
      "test rse 0.7526 | test rae 0.6721 | test corr 0.2582\n",
      "| end of epoch  51 | time:  0.19s | train_loss 9.1676 | valid rse 0.8004 | valid rae 0.7209 | valid corr  0.2290\n",
      "| end of epoch  52 | time:  0.19s | train_loss 9.1654 | valid rse 0.7983 | valid rae 0.7201 | valid corr  0.2310\n",
      "| end of epoch  53 | time:  0.26s | train_loss 9.1645 | valid rse 0.7984 | valid rae 0.7201 | valid corr  0.2329\n",
      "| end of epoch  54 | time:  0.18s | train_loss 9.1572 | valid rse 0.7969 | valid rae 0.7189 | valid corr  0.2322\n",
      "| end of epoch  55 | time:  0.19s | train_loss 9.1486 | valid rse 0.7983 | valid rae 0.7201 | valid corr  0.2312\n",
      "test rse 0.7495 | test rae 0.6694 | test corr 0.2622\n",
      "| end of epoch  56 | time:  0.22s | train_loss 9.1485 | valid rse 0.8006 | valid rae 0.7218 | valid corr  0.2317\n",
      "| end of epoch  57 | time:  0.18s | train_loss 9.1520 | valid rse 0.7979 | valid rae 0.7195 | valid corr  0.2310\n",
      "| end of epoch  58 | time:  0.20s | train_loss 9.1440 | valid rse 0.8013 | valid rae 0.7221 | valid corr  0.2332\n",
      "| end of epoch  59 | time:  0.21s | train_loss 9.1386 | valid rse 0.7979 | valid rae 0.7196 | valid corr  0.2325\n",
      "| end of epoch  60 | time:  0.23s | train_loss 9.1356 | valid rse 0.7979 | valid rae 0.7200 | valid corr  0.2333\n",
      "test rse 0.7507 | test rae 0.6725 | test corr 0.2611\n",
      "| end of epoch  61 | time:  0.20s | train_loss 9.1294 | valid rse 0.8003 | valid rae 0.7223 | valid corr  0.2334\n",
      "| end of epoch  62 | time:  0.19s | train_loss 9.1343 | valid rse 0.7986 | valid rae 0.7205 | valid corr  0.2341\n",
      "| end of epoch  63 | time:  0.19s | train_loss 9.1235 | valid rse 0.7967 | valid rae 0.7190 | valid corr  0.2346\n",
      "| end of epoch  64 | time:  0.19s | train_loss 9.1235 | valid rse 0.7973 | valid rae 0.7203 | valid corr  0.2351\n",
      "| end of epoch  65 | time:  0.23s | train_loss 9.1173 | valid rse 0.7968 | valid rae 0.7205 | valid corr  0.2321\n",
      "test rse 0.7497 | test rae 0.6746 | test corr 0.2609\n",
      "| end of epoch  66 | time:  0.19s | train_loss 9.1308 | valid rse 0.7970 | valid rae 0.7206 | valid corr  0.2325\n",
      "| end of epoch  67 | time:  0.21s | train_loss 9.1242 | valid rse 0.7960 | valid rae 0.7192 | valid corr  0.2344\n",
      "| end of epoch  68 | time:  0.19s | train_loss 9.1216 | valid rse 0.7960 | valid rae 0.7188 | valid corr  0.2345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch  69 | time:  0.18s | train_loss 9.1177 | valid rse 0.7932 | valid rae 0.7173 | valid corr  0.2379\n",
      "| end of epoch  70 | time:  0.22s | train_loss 9.1157 | valid rse 0.7939 | valid rae 0.7189 | valid corr  0.2377\n",
      "test rse 0.7473 | test rae 0.6729 | test corr 0.2649\n",
      "| end of epoch  71 | time:  0.18s | train_loss 9.1060 | valid rse 0.7923 | valid rae 0.7167 | valid corr  0.2388\n",
      "| end of epoch  72 | time:  0.19s | train_loss 9.0983 | valid rse 0.7934 | valid rae 0.7180 | valid corr  0.2369\n",
      "| end of epoch  73 | time:  0.18s | train_loss 9.0959 | valid rse 0.7927 | valid rae 0.7170 | valid corr  0.2389\n",
      "| end of epoch  74 | time:  0.19s | train_loss 9.0911 | valid rse 0.7929 | valid rae 0.7169 | valid corr  0.2372\n",
      "| end of epoch  75 | time:  0.18s | train_loss 9.0876 | valid rse 0.7918 | valid rae 0.7162 | valid corr  0.2386\n",
      "test rse 0.7481 | test rae 0.6721 | test corr 0.2617\n",
      "| end of epoch  76 | time:  0.19s | train_loss 9.0825 | valid rse 0.7927 | valid rae 0.7173 | valid corr  0.2364\n",
      "| end of epoch  77 | time:  0.24s | train_loss 9.0803 | valid rse 0.7912 | valid rae 0.7163 | valid corr  0.2365\n",
      "| end of epoch  78 | time:  0.18s | train_loss 9.0718 | valid rse 0.7924 | valid rae 0.7184 | valid corr  0.2369\n",
      "| end of epoch  79 | time:  0.18s | train_loss 9.0732 | valid rse 0.7945 | valid rae 0.7202 | valid corr  0.2365\n",
      "| end of epoch  80 | time:  0.19s | train_loss 9.0774 | valid rse 0.7963 | valid rae 0.7215 | valid corr  0.2368\n",
      "test rse 0.7491 | test rae 0.6766 | test corr 0.2679\n",
      "| end of epoch  81 | time:  0.22s | train_loss 9.0789 | valid rse 0.7913 | valid rae 0.7168 | valid corr  0.2386\n",
      "| end of epoch  82 | time:  0.22s | train_loss 9.0742 | valid rse 0.7911 | valid rae 0.7161 | valid corr  0.2397\n",
      "| end of epoch  83 | time:  0.21s | train_loss 9.0559 | valid rse 0.7958 | valid rae 0.7218 | valid corr  0.2378\n",
      "| end of epoch  84 | time:  0.20s | train_loss 9.0775 | valid rse 0.7962 | valid rae 0.7220 | valid corr  0.2392\n",
      "| end of epoch  85 | time:  0.18s | train_loss 9.0565 | valid rse 0.7900 | valid rae 0.7161 | valid corr  0.2376\n",
      "test rse 0.7493 | test rae 0.6741 | test corr 0.2631\n",
      "| end of epoch  86 | time:  0.18s | train_loss 9.0570 | valid rse 0.7930 | valid rae 0.7181 | valid corr  0.2398\n",
      "| end of epoch  87 | time:  0.20s | train_loss 9.0463 | valid rse 0.7911 | valid rae 0.7174 | valid corr  0.2432\n",
      "| end of epoch  88 | time:  0.18s | train_loss 9.0448 | valid rse 0.7902 | valid rae 0.7172 | valid corr  0.2435\n",
      "| end of epoch  89 | time:  0.18s | train_loss 9.0385 | valid rse 0.7881 | valid rae 0.7145 | valid corr  0.2463\n",
      "| end of epoch  90 | time:  0.18s | train_loss 9.0245 | valid rse 0.7863 | valid rae 0.7136 | valid corr  0.2473\n",
      "test rse 0.7453 | test rae 0.6711 | test corr 0.2707\n",
      "| end of epoch  91 | time:  0.18s | train_loss 9.0289 | valid rse 0.7883 | valid rae 0.7155 | valid corr  0.2451\n",
      "| end of epoch  92 | time:  0.18s | train_loss 9.0195 | valid rse 0.7900 | valid rae 0.7183 | valid corr  0.2441\n",
      "| end of epoch  93 | time:  0.18s | train_loss 9.0112 | valid rse 0.7877 | valid rae 0.7145 | valid corr  0.2460\n",
      "| end of epoch  94 | time:  0.19s | train_loss 9.0088 | valid rse 0.7877 | valid rae 0.7143 | valid corr  0.2461\n",
      "| end of epoch  95 | time:  0.20s | train_loss 9.0030 | valid rse 0.7898 | valid rae 0.7177 | valid corr  0.2444\n",
      "test rse 0.7443 | test rae 0.6743 | test corr 0.2704\n",
      "| end of epoch  96 | time:  0.19s | train_loss 9.0135 | valid rse 0.7852 | valid rae 0.7144 | valid corr  0.2457\n",
      "| end of epoch  97 | time:  0.21s | train_loss 9.0021 | valid rse 0.7845 | valid rae 0.7123 | valid corr  0.2468\n",
      "| end of epoch  98 | time:  0.17s | train_loss 8.9948 | valid rse 0.7853 | valid rae 0.7127 | valid corr  0.2503\n",
      "| end of epoch  99 | time:  0.18s | train_loss 8.9866 | valid rse 0.7856 | valid rae 0.7141 | valid corr  0.2517\n",
      "| end of epoch 100 | time:  0.19s | train_loss 8.9848 | valid rse 0.7877 | valid rae 0.7140 | valid corr  0.2478\n",
      "test rse 0.7500 | test rae 0.6749 | test corr 0.2708\n",
      "| end of epoch 101 | time:  0.21s | train_loss 8.9879 | valid rse 0.7885 | valid rae 0.7146 | valid corr  0.2486\n",
      "| end of epoch 102 | time:  0.21s | train_loss 8.9748 | valid rse 0.7865 | valid rae 0.7148 | valid corr  0.2504\n",
      "| end of epoch 103 | time:  0.18s | train_loss 8.9742 | valid rse 0.7891 | valid rae 0.7167 | valid corr  0.2513\n",
      "| end of epoch 104 | time:  0.19s | train_loss 8.9737 | valid rse 0.7853 | valid rae 0.7123 | valid corr  0.2547\n",
      "| end of epoch 105 | time:  0.18s | train_loss 8.9788 | valid rse 0.7855 | valid rae 0.7133 | valid corr  0.2570\n",
      "test rse 0.7460 | test rae 0.6731 | test corr 0.2761\n",
      "| end of epoch 106 | time:  0.19s | train_loss 8.9640 | valid rse 0.7869 | valid rae 0.7142 | valid corr  0.2566\n",
      "| end of epoch 107 | time:  0.22s | train_loss 8.9632 | valid rse 0.7846 | valid rae 0.7118 | valid corr  0.2550\n",
      "| end of epoch 108 | time:  0.18s | train_loss 8.9626 | valid rse 0.7880 | valid rae 0.7143 | valid corr  0.2565\n",
      "| end of epoch 109 | time:  0.18s | train_loss 8.9553 | valid rse 0.7851 | valid rae 0.7119 | valid corr  0.2550\n",
      "| end of epoch 110 | time:  0.18s | train_loss 8.9555 | valid rse 0.7905 | valid rae 0.7146 | valid corr  0.2523\n",
      "test rse 0.7508 | test rae 0.6740 | test corr 0.2713\n",
      "| end of epoch 111 | time:  0.17s | train_loss 8.9474 | valid rse 0.7893 | valid rae 0.7150 | valid corr  0.2530\n",
      "| end of epoch 112 | time:  0.18s | train_loss 8.9372 | valid rse 0.7894 | valid rae 0.7159 | valid corr  0.2594\n",
      "| end of epoch 113 | time:  0.17s | train_loss 8.9371 | valid rse 0.7869 | valid rae 0.7149 | valid corr  0.2565\n",
      "| end of epoch 114 | time:  0.18s | train_loss 8.9499 | valid rse 0.7887 | valid rae 0.7161 | valid corr  0.2625\n",
      "| end of epoch 115 | time:  0.17s | train_loss 8.9407 | valid rse 0.7888 | valid rae 0.7168 | valid corr  0.2609\n",
      "test rse 0.7462 | test rae 0.6768 | test corr 0.2785\n",
      "| end of epoch 116 | time:  0.18s | train_loss 8.9300 | valid rse 0.7891 | valid rae 0.7156 | valid corr  0.2594\n",
      "| end of epoch 117 | time:  0.21s | train_loss 8.9349 | valid rse 0.7818 | valid rae 0.7093 | valid corr  0.2567\n",
      "| end of epoch 118 | time:  0.21s | train_loss 8.9213 | valid rse 0.7858 | valid rae 0.7128 | valid corr  0.2591\n",
      "| end of epoch 119 | time:  0.20s | train_loss 8.9067 | valid rse 0.7875 | valid rae 0.7156 | valid corr  0.2559\n",
      "| end of epoch 120 | time:  0.18s | train_loss 8.9080 | valid rse 0.7819 | valid rae 0.7114 | valid corr  0.2631\n",
      "test rse 0.7445 | test rae 0.6752 | test corr 0.2758\n",
      "| end of epoch 121 | time:  0.19s | train_loss 8.9109 | valid rse 0.7858 | valid rae 0.7136 | valid corr  0.2602\n",
      "| end of epoch 122 | time:  0.18s | train_loss 8.9062 | valid rse 0.7820 | valid rae 0.7098 | valid corr  0.2597\n",
      "| end of epoch 123 | time:  0.18s | train_loss 8.8944 | valid rse 0.7800 | valid rae 0.7087 | valid corr  0.2620\n",
      "| end of epoch 124 | time:  0.20s | train_loss 8.8933 | valid rse 0.7818 | valid rae 0.7096 | valid corr  0.2625\n",
      "| end of epoch 125 | time:  0.22s | train_loss 8.8984 | valid rse 0.7850 | valid rae 0.7149 | valid corr  0.2596\n",
      "test rse 0.7421 | test rae 0.6748 | test corr 0.2755\n",
      "| end of epoch 126 | time:  0.20s | train_loss 8.8918 | valid rse 0.7901 | valid rae 0.7180 | valid corr  0.2610\n",
      "| end of epoch 127 | time:  0.19s | train_loss 8.8834 | valid rse 0.7820 | valid rae 0.7113 | valid corr  0.2600\n",
      "| end of epoch 128 | time:  0.18s | train_loss 8.8927 | valid rse 0.7799 | valid rae 0.7091 | valid corr  0.2657\n",
      "| end of epoch 129 | time:  0.19s | train_loss 8.8843 | valid rse 0.7784 | valid rae 0.7091 | valid corr  0.2644\n",
      "| end of epoch 130 | time:  0.17s | train_loss 8.8712 | valid rse 0.7832 | valid rae 0.7111 | valid corr  0.2650\n",
      "test rse 0.7487 | test rae 0.6768 | test corr 0.2759\n",
      "| end of epoch 131 | time:  0.19s | train_loss 8.8794 | valid rse 0.7799 | valid rae 0.7089 | valid corr  0.2590\n",
      "| end of epoch 132 | time:  0.21s | train_loss 8.8745 | valid rse 0.7842 | valid rae 0.7117 | valid corr  0.2610\n",
      "| end of epoch 133 | time:  0.19s | train_loss 8.8798 | valid rse 0.7804 | valid rae 0.7103 | valid corr  0.2633\n",
      "| end of epoch 134 | time:  0.20s | train_loss 8.8701 | valid rse 0.7835 | valid rae 0.7130 | valid corr  0.2652\n",
      "| end of epoch 135 | time:  0.20s | train_loss 8.8731 | valid rse 0.7780 | valid rae 0.7084 | valid corr  0.2601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rse 0.7405 | test rae 0.6720 | test corr 0.2782\n",
      "| end of epoch 136 | time:  0.18s | train_loss 8.8644 | valid rse 0.7827 | valid rae 0.7120 | valid corr  0.2641\n",
      "| end of epoch 137 | time:  0.19s | train_loss 8.8637 | valid rse 0.7772 | valid rae 0.7079 | valid corr  0.2619\n",
      "| end of epoch 138 | time:  0.21s | train_loss 8.8506 | valid rse 0.7834 | valid rae 0.7124 | valid corr  0.2635\n",
      "| end of epoch 139 | time:  0.20s | train_loss 8.8455 | valid rse 0.7853 | valid rae 0.7141 | valid corr  0.2643\n",
      "| end of epoch 140 | time:  0.21s | train_loss 8.8493 | valid rse 0.7802 | valid rae 0.7091 | valid corr  0.2627\n",
      "test rse 0.7459 | test rae 0.6761 | test corr 0.2689\n",
      "| end of epoch 141 | time:  0.21s | train_loss 8.8383 | valid rse 0.7825 | valid rae 0.7132 | valid corr  0.2646\n",
      "| end of epoch 142 | time:  0.19s | train_loss 8.8345 | valid rse 0.7862 | valid rae 0.7170 | valid corr  0.2656\n",
      "| end of epoch 143 | time:  0.19s | train_loss 8.8412 | valid rse 0.7779 | valid rae 0.7083 | valid corr  0.2662\n",
      "| end of epoch 144 | time:  0.21s | train_loss 8.8435 | valid rse 0.7765 | valid rae 0.7072 | valid corr  0.2689\n",
      "| end of epoch 145 | time:  0.18s | train_loss 8.8424 | valid rse 0.7759 | valid rae 0.7081 | valid corr  0.2673\n",
      "test rse 0.7411 | test rae 0.6747 | test corr 0.2772\n",
      "| end of epoch 146 | time:  0.17s | train_loss 8.8393 | valid rse 0.7815 | valid rae 0.7125 | valid corr  0.2648\n",
      "| end of epoch 147 | time:  0.19s | train_loss 8.8289 | valid rse 0.7812 | valid rae 0.7106 | valid corr  0.2612\n",
      "| end of epoch 148 | time:  0.22s | train_loss 8.8277 | valid rse 0.7792 | valid rae 0.7081 | valid corr  0.2607\n",
      "| end of epoch 149 | time:  0.20s | train_loss 8.8234 | valid rse 0.7865 | valid rae 0.7147 | valid corr  0.2653\n",
      "| end of epoch 150 | time:  0.18s | train_loss 8.8208 | valid rse 0.7807 | valid rae 0.7121 | valid corr  0.2694\n",
      "test rse 0.7464 | test rae 0.6811 | test corr 0.2764\n",
      "| end of epoch 151 | time:  0.19s | train_loss 8.8210 | valid rse 0.7797 | valid rae 0.7103 | valid corr  0.2663\n",
      "| end of epoch 152 | time:  0.20s | train_loss 8.8107 | valid rse 0.7790 | valid rae 0.7094 | valid corr  0.2685\n",
      "| end of epoch 153 | time:  0.19s | train_loss 8.8217 | valid rse 0.7769 | valid rae 0.7085 | valid corr  0.2681\n",
      "| end of epoch 154 | time:  0.19s | train_loss 8.8031 | valid rse 0.7803 | valid rae 0.7109 | valid corr  0.2704\n",
      "| end of epoch 155 | time:  0.18s | train_loss 8.8011 | valid rse 0.7787 | valid rae 0.7089 | valid corr  0.2686\n",
      "test rse 0.7457 | test rae 0.6775 | test corr 0.2763\n",
      "| end of epoch 156 | time:  0.18s | train_loss 8.8013 | valid rse 0.7789 | valid rae 0.7085 | valid corr  0.2680\n",
      "| end of epoch 157 | time:  0.18s | train_loss 8.8075 | valid rse 0.7795 | valid rae 0.7093 | valid corr  0.2682\n",
      "| end of epoch 158 | time:  0.22s | train_loss 8.7986 | valid rse 0.7798 | valid rae 0.7113 | valid corr  0.2707\n",
      "| end of epoch 159 | time:  0.19s | train_loss 8.7972 | valid rse 0.7788 | valid rae 0.7103 | valid corr  0.2671\n",
      "| end of epoch 160 | time:  0.19s | train_loss 8.7935 | valid rse 0.7797 | valid rae 0.7122 | valid corr  0.2641\n",
      "test rse 0.7499 | test rae 0.6866 | test corr 0.2697\n",
      "| end of epoch 161 | time:  0.20s | train_loss 8.7953 | valid rse 0.7781 | valid rae 0.7124 | valid corr  0.2668\n",
      "| end of epoch 162 | time:  0.22s | train_loss 8.7907 | valid rse 0.7805 | valid rae 0.7114 | valid corr  0.2678\n",
      "| end of epoch 163 | time:  0.18s | train_loss 8.7924 | valid rse 0.7784 | valid rae 0.7090 | valid corr  0.2663\n",
      "| end of epoch 164 | time:  0.21s | train_loss 8.7761 | valid rse 0.7804 | valid rae 0.7104 | valid corr  0.2688\n",
      "| end of epoch 165 | time:  0.20s | train_loss 8.7771 | valid rse 0.7760 | valid rae 0.7082 | valid corr  0.2678\n",
      "test rse 0.7449 | test rae 0.6788 | test corr 0.2716\n",
      "| end of epoch 166 | time:  0.20s | train_loss 8.7739 | valid rse 0.7772 | valid rae 0.7093 | valid corr  0.2676\n",
      "| end of epoch 167 | time:  0.20s | train_loss 8.7688 | valid rse 0.7744 | valid rae 0.7099 | valid corr  0.2654\n",
      "| end of epoch 168 | time:  0.18s | train_loss 8.7904 | valid rse 0.7814 | valid rae 0.7142 | valid corr  0.2657\n",
      "| end of epoch 169 | time:  0.18s | train_loss 8.7669 | valid rse 0.7756 | valid rae 0.7080 | valid corr  0.2654\n",
      "| end of epoch 170 | time:  0.21s | train_loss 8.7694 | valid rse 0.7759 | valid rae 0.7071 | valid corr  0.2676\n",
      "test rse 0.7447 | test rae 0.6766 | test corr 0.2775\n",
      "| end of epoch 171 | time:  0.21s | train_loss 8.7568 | valid rse 0.7762 | valid rae 0.7076 | valid corr  0.2669\n",
      "| end of epoch 172 | time:  0.20s | train_loss 8.7576 | valid rse 0.7768 | valid rae 0.7077 | valid corr  0.2713\n",
      "| end of epoch 173 | time:  0.20s | train_loss 8.7506 | valid rse 0.7733 | valid rae 0.7064 | valid corr  0.2713\n",
      "| end of epoch 174 | time:  0.19s | train_loss 8.7410 | valid rse 0.7797 | valid rae 0.7100 | valid corr  0.2695\n",
      "| end of epoch 175 | time:  0.22s | train_loss 8.7462 | valid rse 0.7784 | valid rae 0.7088 | valid corr  0.2685\n",
      "test rse 0.7477 | test rae 0.6818 | test corr 0.2727\n",
      "| end of epoch 176 | time:  0.19s | train_loss 8.7296 | valid rse 0.7795 | valid rae 0.7092 | valid corr  0.2671\n",
      "| end of epoch 177 | time:  0.18s | train_loss 8.7309 | valid rse 0.7795 | valid rae 0.7103 | valid corr  0.2648\n",
      "| end of epoch 178 | time:  0.18s | train_loss 8.7345 | valid rse 0.7815 | valid rae 0.7115 | valid corr  0.2701\n",
      "| end of epoch 179 | time:  0.19s | train_loss 8.7248 | valid rse 0.7769 | valid rae 0.7096 | valid corr  0.2737\n",
      "| end of epoch 180 | time:  0.21s | train_loss 8.7145 | valid rse 0.7770 | valid rae 0.7092 | valid corr  0.2727\n",
      "test rse 0.7478 | test rae 0.6846 | test corr 0.2722\n",
      "| end of epoch 181 | time:  0.19s | train_loss 8.7303 | valid rse 0.7802 | valid rae 0.7116 | valid corr  0.2712\n",
      "| end of epoch 182 | time:  0.21s | train_loss 8.7253 | valid rse 0.7770 | valid rae 0.7079 | valid corr  0.2690\n",
      "| end of epoch 183 | time:  0.22s | train_loss 8.7093 | valid rse 0.7781 | valid rae 0.7098 | valid corr  0.2711\n",
      "| end of epoch 184 | time:  0.19s | train_loss 8.7026 | valid rse 0.7792 | valid rae 0.7125 | valid corr  0.2671\n",
      "| end of epoch 185 | time:  0.22s | train_loss 8.7081 | valid rse 0.7873 | valid rae 0.7159 | valid corr  0.2661\n",
      "test rse 0.7522 | test rae 0.6869 | test corr 0.2707\n",
      "| end of epoch 186 | time:  0.19s | train_loss 8.7192 | valid rse 0.7835 | valid rae 0.7126 | valid corr  0.2671\n",
      "| end of epoch 187 | time:  0.21s | train_loss 8.7129 | valid rse 0.7748 | valid rae 0.7074 | valid corr  0.2688\n",
      "| end of epoch 188 | time:  0.21s | train_loss 8.7191 | valid rse 0.7763 | valid rae 0.7086 | valid corr  0.2734\n",
      "| end of epoch 189 | time:  0.21s | train_loss 8.7093 | valid rse 0.7797 | valid rae 0.7114 | valid corr  0.2714\n",
      "| end of epoch 190 | time:  0.21s | train_loss 8.6976 | valid rse 0.7767 | valid rae 0.7079 | valid corr  0.2716\n",
      "test rse 0.7489 | test rae 0.6843 | test corr 0.2733\n",
      "| end of epoch 191 | time:  0.21s | train_loss 8.6869 | valid rse 0.7743 | valid rae 0.7069 | valid corr  0.2742\n",
      "| end of epoch 192 | time:  0.18s | train_loss 8.6787 | valid rse 0.7714 | valid rae 0.7062 | valid corr  0.2706\n",
      "| end of epoch 193 | time:  0.18s | train_loss 8.6859 | valid rse 0.7755 | valid rae 0.7076 | valid corr  0.2726\n",
      "| end of epoch 194 | time:  0.18s | train_loss 8.6855 | valid rse 0.7773 | valid rae 0.7095 | valid corr  0.2735\n",
      "| end of epoch 195 | time:  0.19s | train_loss 8.6739 | valid rse 0.7773 | valid rae 0.7099 | valid corr  0.2720\n",
      "test rse 0.7482 | test rae 0.6878 | test corr 0.2739\n",
      "| end of epoch 196 | time:  0.18s | train_loss 8.6805 | valid rse 0.7748 | valid rae 0.7074 | valid corr  0.2709\n",
      "| end of epoch 197 | time:  0.18s | train_loss 8.6839 | valid rse 0.7762 | valid rae 0.7123 | valid corr  0.2670\n",
      "| end of epoch 198 | time:  0.21s | train_loss 8.6986 | valid rse 0.7770 | valid rae 0.7115 | valid corr  0.2699\n",
      "| end of epoch 199 | time:  0.23s | train_loss 8.6818 | valid rse 0.7715 | valid rae 0.7067 | valid corr  0.2710\n",
      "| end of epoch 200 | time:  0.19s | train_loss 8.6612 | valid rse 0.7717 | valid rae 0.7064 | valid corr  0.2731\n",
      "test rse 0.7484 | test rae 0.6877 | test corr 0.2717\n",
      "| end of epoch 201 | time:  0.18s | train_loss 8.6618 | valid rse 0.7731 | valid rae 0.7080 | valid corr  0.2733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch 202 | time:  0.20s | train_loss 8.6497 | valid rse 0.7724 | valid rae 0.7063 | valid corr  0.2738\n",
      "| end of epoch 203 | time:  0.17s | train_loss 8.6548 | valid rse 0.7704 | valid rae 0.7074 | valid corr  0.2706\n",
      "| end of epoch 204 | time:  0.19s | train_loss 8.6579 | valid rse 0.7729 | valid rae 0.7078 | valid corr  0.2718\n",
      "| end of epoch 205 | time:  0.19s | train_loss 8.6439 | valid rse 0.7778 | valid rae 0.7099 | valid corr  0.2719\n",
      "test rse 0.7516 | test rae 0.6904 | test corr 0.2699\n",
      "| end of epoch 206 | time:  0.21s | train_loss 8.6499 | valid rse 0.7770 | valid rae 0.7091 | valid corr  0.2680\n",
      "| end of epoch 207 | time:  0.20s | train_loss 8.6386 | valid rse 0.7727 | valid rae 0.7071 | valid corr  0.2707\n",
      "| end of epoch 208 | time:  0.21s | train_loss 8.6328 | valid rse 0.7727 | valid rae 0.7072 | valid corr  0.2760\n",
      "| end of epoch 209 | time:  0.19s | train_loss 8.6401 | valid rse 0.7675 | valid rae 0.7049 | valid corr  0.2754\n",
      "| end of epoch 210 | time:  0.18s | train_loss 8.6277 | valid rse 0.7702 | valid rae 0.7057 | valid corr  0.2768\n",
      "test rse 0.7499 | test rae 0.6882 | test corr 0.2765\n",
      "| end of epoch 211 | time:  0.19s | train_loss 8.6251 | valid rse 0.7746 | valid rae 0.7084 | valid corr  0.2769\n",
      "| end of epoch 212 | time:  0.22s | train_loss 8.6279 | valid rse 0.7803 | valid rae 0.7119 | valid corr  0.2703\n",
      "| end of epoch 213 | time:  0.22s | train_loss 8.6283 | valid rse 0.7793 | valid rae 0.7127 | valid corr  0.2723\n",
      "| end of epoch 214 | time:  0.21s | train_loss 8.6172 | valid rse 0.7781 | valid rae 0.7117 | valid corr  0.2736\n",
      "| end of epoch 215 | time:  0.19s | train_loss 8.6218 | valid rse 0.7740 | valid rae 0.7077 | valid corr  0.2769\n",
      "test rse 0.7490 | test rae 0.6857 | test corr 0.2789\n",
      "| end of epoch 216 | time:  0.18s | train_loss 8.6254 | valid rse 0.7709 | valid rae 0.7078 | valid corr  0.2740\n",
      "| end of epoch 217 | time:  0.19s | train_loss 8.6130 | valid rse 0.7749 | valid rae 0.7106 | valid corr  0.2722\n",
      "| end of epoch 218 | time:  0.19s | train_loss 8.6146 | valid rse 0.7773 | valid rae 0.7105 | valid corr  0.2731\n",
      "| end of epoch 219 | time:  0.19s | train_loss 8.6026 | valid rse 0.7794 | valid rae 0.7127 | valid corr  0.2678\n",
      "| end of epoch 220 | time:  0.18s | train_loss 8.5932 | valid rse 0.7759 | valid rae 0.7098 | valid corr  0.2750\n",
      "test rse 0.7546 | test rae 0.6940 | test corr 0.2634\n",
      "| end of epoch 221 | time:  0.19s | train_loss 8.5926 | valid rse 0.7797 | valid rae 0.7129 | valid corr  0.2738\n",
      "| end of epoch 222 | time:  0.19s | train_loss 8.5899 | valid rse 0.7784 | valid rae 0.7106 | valid corr  0.2701\n",
      "| end of epoch 223 | time:  0.20s | train_loss 8.5881 | valid rse 0.7721 | valid rae 0.7093 | valid corr  0.2744\n",
      "| end of epoch 224 | time:  0.19s | train_loss 8.5893 | valid rse 0.7794 | valid rae 0.7162 | valid corr  0.2737\n",
      "| end of epoch 225 | time:  0.19s | train_loss 8.5869 | valid rse 0.7740 | valid rae 0.7103 | valid corr  0.2694\n",
      "test rse 0.7524 | test rae 0.6954 | test corr 0.2627\n",
      "| end of epoch 226 | time:  0.20s | train_loss 8.5720 | valid rse 0.7769 | valid rae 0.7103 | valid corr  0.2686\n",
      "| end of epoch 227 | time:  0.20s | train_loss 8.5882 | valid rse 0.7719 | valid rae 0.7086 | valid corr  0.2705\n",
      "| end of epoch 228 | time:  0.22s | train_loss 8.5816 | valid rse 0.7727 | valid rae 0.7101 | valid corr  0.2740\n",
      "| end of epoch 229 | time:  0.21s | train_loss 8.5686 | valid rse 0.7736 | valid rae 0.7088 | valid corr  0.2700\n",
      "| end of epoch 230 | time:  0.19s | train_loss 8.5839 | valid rse 0.7792 | valid rae 0.7121 | valid corr  0.2691\n",
      "test rse 0.7608 | test rae 0.6983 | test corr 0.2561\n",
      "| end of epoch 231 | time:  0.21s | train_loss 8.5823 | valid rse 0.7748 | valid rae 0.7082 | valid corr  0.2755\n",
      "| end of epoch 232 | time:  0.19s | train_loss 8.5707 | valid rse 0.7779 | valid rae 0.7094 | valid corr  0.2727\n",
      "| end of epoch 233 | time:  0.19s | train_loss 8.5614 | valid rse 0.7832 | valid rae 0.7147 | valid corr  0.2730\n",
      "| end of epoch 234 | time:  0.19s | train_loss 8.5532 | valid rse 0.7806 | valid rae 0.7128 | valid corr  0.2704\n",
      "| end of epoch 235 | time:  0.20s | train_loss 8.5589 | valid rse 0.7696 | valid rae 0.7067 | valid corr  0.2782\n",
      "test rse 0.7509 | test rae 0.6919 | test corr 0.2651\n",
      "| end of epoch 236 | time:  0.18s | train_loss 8.5577 | valid rse 0.7725 | valid rae 0.7084 | valid corr  0.2734\n",
      "| end of epoch 237 | time:  0.19s | train_loss 8.5583 | valid rse 0.7757 | valid rae 0.7107 | valid corr  0.2688\n",
      "| end of epoch 238 | time:  0.18s | train_loss 8.5489 | valid rse 0.7712 | valid rae 0.7070 | valid corr  0.2760\n",
      "| end of epoch 239 | time:  0.19s | train_loss 8.5346 | valid rse 0.7707 | valid rae 0.7087 | valid corr  0.2761\n",
      "| end of epoch 240 | time:  0.19s | train_loss 8.5471 | valid rse 0.7757 | valid rae 0.7114 | valid corr  0.2776\n",
      "test rse 0.7619 | test rae 0.7026 | test corr 0.2614\n",
      "| end of epoch 241 | time:  0.22s | train_loss 8.5359 | valid rse 0.7713 | valid rae 0.7084 | valid corr  0.2762\n",
      "| end of epoch 242 | time:  0.18s | train_loss 8.5313 | valid rse 0.7779 | valid rae 0.7123 | valid corr  0.2725\n",
      "| end of epoch 243 | time:  0.18s | train_loss 8.5380 | valid rse 0.7844 | valid rae 0.7166 | valid corr  0.2656\n",
      "| end of epoch 244 | time:  0.18s | train_loss 8.5256 | valid rse 0.7803 | valid rae 0.7131 | valid corr  0.2688\n",
      "| end of epoch 245 | time:  0.17s | train_loss 8.5239 | valid rse 0.7763 | valid rae 0.7099 | valid corr  0.2798\n",
      "test rse 0.7596 | test rae 0.6983 | test corr 0.2648\n",
      "| end of epoch 246 | time:  0.18s | train_loss 8.5292 | valid rse 0.7724 | valid rae 0.7074 | valid corr  0.2789\n",
      "| end of epoch 247 | time:  0.18s | train_loss 8.5240 | valid rse 0.7713 | valid rae 0.7103 | valid corr  0.2774\n",
      "| end of epoch 248 | time:  0.17s | train_loss 8.5219 | valid rse 0.7740 | valid rae 0.7109 | valid corr  0.2730\n",
      "| end of epoch 249 | time:  0.20s | train_loss 8.5169 | valid rse 0.7763 | valid rae 0.7125 | valid corr  0.2771\n",
      "| end of epoch 250 | time:  0.20s | train_loss 8.5217 | valid rse 0.7704 | valid rae 0.7089 | valid corr  0.2816\n",
      "test rse 0.7625 | test rae 0.7062 | test corr 0.2556\n",
      "| end of epoch 251 | time:  0.21s | train_loss 8.5055 | valid rse 0.7693 | valid rae 0.7078 | valid corr  0.2758\n",
      "| end of epoch 252 | time:  0.18s | train_loss 8.5097 | valid rse 0.7757 | valid rae 0.7111 | valid corr  0.2733\n",
      "| end of epoch 253 | time:  0.18s | train_loss 8.4898 | valid rse 0.7748 | valid rae 0.7105 | valid corr  0.2756\n",
      "| end of epoch 254 | time:  0.20s | train_loss 8.5037 | valid rse 0.7825 | valid rae 0.7173 | valid corr  0.2744\n",
      "| end of epoch 255 | time:  0.17s | train_loss 8.5122 | valid rse 0.7923 | valid rae 0.7245 | valid corr  0.2667\n",
      "test rse 0.7701 | test rae 0.7143 | test corr 0.2502\n",
      "| end of epoch 256 | time:  0.18s | train_loss 8.5017 | valid rse 0.7814 | valid rae 0.7142 | valid corr  0.2706\n",
      "| end of epoch 257 | time:  0.19s | train_loss 8.4914 | valid rse 0.7724 | valid rae 0.7083 | valid corr  0.2761\n",
      "| end of epoch 258 | time:  0.21s | train_loss 8.5006 | valid rse 0.7680 | valid rae 0.7062 | valid corr  0.2793\n",
      "| end of epoch 259 | time:  0.18s | train_loss 8.5095 | valid rse 0.7706 | valid rae 0.7074 | valid corr  0.2793\n",
      "| end of epoch 260 | time:  0.18s | train_loss 8.4853 | valid rse 0.7747 | valid rae 0.7101 | valid corr  0.2799\n",
      "test rse 0.7596 | test rae 0.7022 | test corr 0.2609\n",
      "| end of epoch 261 | time:  0.18s | train_loss 8.4845 | valid rse 0.7704 | valid rae 0.7082 | valid corr  0.2735\n",
      "| end of epoch 262 | time:  0.18s | train_loss 8.4805 | valid rse 0.7781 | valid rae 0.7134 | valid corr  0.2733\n",
      "| end of epoch 263 | time:  0.18s | train_loss 8.4783 | valid rse 0.7833 | valid rae 0.7177 | valid corr  0.2771\n",
      "| end of epoch 264 | time:  0.18s | train_loss 8.5003 | valid rse 0.7786 | valid rae 0.7117 | valid corr  0.2792\n",
      "| end of epoch 265 | time:  0.18s | train_loss 8.4911 | valid rse 0.7794 | valid rae 0.7130 | valid corr  0.2776\n",
      "test rse 0.7624 | test rae 0.7023 | test corr 0.2595\n",
      "| end of epoch 266 | time:  0.18s | train_loss 8.4785 | valid rse 0.7718 | valid rae 0.7089 | valid corr  0.2775\n",
      "| end of epoch 267 | time:  0.18s | train_loss 8.4928 | valid rse 0.7754 | valid rae 0.7103 | valid corr  0.2705\n",
      "| end of epoch 268 | time:  0.19s | train_loss 8.4973 | valid rse 0.7758 | valid rae 0.7101 | valid corr  0.2809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch 269 | time:  0.20s | train_loss 8.4948 | valid rse 0.7711 | valid rae 0.7066 | valid corr  0.2847\n",
      "| end of epoch 270 | time:  0.18s | train_loss 8.4719 | valid rse 0.7758 | valid rae 0.7118 | valid corr  0.2813\n",
      "test rse 0.7613 | test rae 0.7054 | test corr 0.2544\n",
      "| end of epoch 271 | time:  0.18s | train_loss 8.4691 | valid rse 0.7802 | valid rae 0.7153 | valid corr  0.2745\n",
      "| end of epoch 272 | time:  0.20s | train_loss 8.4680 | valid rse 0.7724 | valid rae 0.7123 | valid corr  0.2735\n",
      "| end of epoch 273 | time:  0.21s | train_loss 8.4620 | valid rse 0.7745 | valid rae 0.7114 | valid corr  0.2776\n",
      "| end of epoch 274 | time:  0.21s | train_loss 8.4616 | valid rse 0.7721 | valid rae 0.7114 | valid corr  0.2822\n",
      "| end of epoch 275 | time:  0.20s | train_loss 8.4525 | valid rse 0.7737 | valid rae 0.7122 | valid corr  0.2820\n",
      "test rse 0.7598 | test rae 0.7028 | test corr 0.2594\n",
      "| end of epoch 276 | time:  0.20s | train_loss 8.4378 | valid rse 0.7774 | valid rae 0.7139 | valid corr  0.2837\n",
      "| end of epoch 277 | time:  0.18s | train_loss 8.4602 | valid rse 0.7749 | valid rae 0.7119 | valid corr  0.2866\n",
      "| end of epoch 278 | time:  0.19s | train_loss 8.4493 | valid rse 0.7728 | valid rae 0.7135 | valid corr  0.2773\n",
      "| end of epoch 279 | time:  0.19s | train_loss 8.4641 | valid rse 0.7765 | valid rae 0.7146 | valid corr  0.2817\n",
      "| end of epoch 280 | time:  0.18s | train_loss 8.4444 | valid rse 0.7650 | valid rae 0.7073 | valid corr  0.2847\n",
      "test rse 0.7594 | test rae 0.7071 | test corr 0.2553\n",
      "| end of epoch 281 | time:  0.18s | train_loss 8.4208 | valid rse 0.7707 | valid rae 0.7088 | valid corr  0.2798\n",
      "| end of epoch 282 | time:  0.18s | train_loss 8.4383 | valid rse 0.7740 | valid rae 0.7114 | valid corr  0.2813\n",
      "| end of epoch 283 | time:  0.18s | train_loss 8.4319 | valid rse 0.7669 | valid rae 0.7060 | valid corr  0.2784\n",
      "| end of epoch 284 | time:  0.18s | train_loss 8.4380 | valid rse 0.7726 | valid rae 0.7098 | valid corr  0.2862\n",
      "| end of epoch 285 | time:  0.18s | train_loss 8.4337 | valid rse 0.7789 | valid rae 0.7162 | valid corr  0.2823\n",
      "test rse 0.7606 | test rae 0.7069 | test corr 0.2667\n",
      "| end of epoch 286 | time:  0.18s | train_loss 8.4257 | valid rse 0.7681 | valid rae 0.7058 | valid corr  0.2887\n",
      "| end of epoch 287 | time:  0.19s | train_loss 8.4152 | valid rse 0.7722 | valid rae 0.7109 | valid corr  0.2824\n",
      "| end of epoch 288 | time:  0.20s | train_loss 8.4433 | valid rse 0.7818 | valid rae 0.7178 | valid corr  0.2812\n",
      "| end of epoch 289 | time:  0.20s | train_loss 8.4451 | valid rse 0.7806 | valid rae 0.7160 | valid corr  0.2877\n",
      "| end of epoch 290 | time:  0.18s | train_loss 8.4407 | valid rse 0.7786 | valid rae 0.7143 | valid corr  0.2846\n",
      "test rse 0.7624 | test rae 0.7092 | test corr 0.2662\n",
      "| end of epoch 291 | time:  0.18s | train_loss 8.4168 | valid rse 0.7672 | valid rae 0.7082 | valid corr  0.2810\n",
      "| end of epoch 292 | time:  0.18s | train_loss 8.4237 | valid rse 0.7681 | valid rae 0.7070 | valid corr  0.2869\n",
      "| end of epoch 293 | time:  0.21s | train_loss 8.4295 | valid rse 0.7687 | valid rae 0.7079 | valid corr  0.2846\n",
      "| end of epoch 294 | time:  0.18s | train_loss 8.4115 | valid rse 0.7736 | valid rae 0.7119 | valid corr  0.2817\n",
      "| end of epoch 295 | time:  0.21s | train_loss 8.4290 | valid rse 0.7810 | valid rae 0.7170 | valid corr  0.2841\n",
      "test rse 0.7691 | test rae 0.7153 | test corr 0.2567\n",
      "| end of epoch 296 | time:  0.22s | train_loss 8.4014 | valid rse 0.7664 | valid rae 0.7050 | valid corr  0.2891\n",
      "| end of epoch 297 | time:  0.21s | train_loss 8.4293 | valid rse 0.7659 | valid rae 0.7077 | valid corr  0.2869\n",
      "| end of epoch 298 | time:  0.20s | train_loss 8.4079 | valid rse 0.7782 | valid rae 0.7142 | valid corr  0.2816\n",
      "| end of epoch 299 | time:  0.22s | train_loss 8.4080 | valid rse 0.7819 | valid rae 0.7187 | valid corr  0.2724\n",
      "| end of epoch 300 | time:  0.20s | train_loss 8.3927 | valid rse 0.7714 | valid rae 0.7100 | valid corr  0.2820\n",
      "test rse 0.7671 | test rae 0.7114 | test corr 0.2576\n",
      "| end of epoch 301 | time:  0.25s | train_loss 8.4171 | valid rse 0.7641 | valid rae 0.7059 | valid corr  0.2786\n",
      "| end of epoch 302 | time:  0.20s | train_loss 8.3909 | valid rse 0.7726 | valid rae 0.7111 | valid corr  0.2849\n",
      "| end of epoch 303 | time:  0.19s | train_loss 8.3877 | valid rse 0.7840 | valid rae 0.7200 | valid corr  0.2763\n",
      "| end of epoch 304 | time:  0.19s | train_loss 8.3876 | valid rse 0.7804 | valid rae 0.7146 | valid corr  0.2727\n",
      "| end of epoch 305 | time:  0.19s | train_loss 8.3846 | valid rse 0.7671 | valid rae 0.7064 | valid corr  0.2827\n",
      "test rse 0.7634 | test rae 0.7093 | test corr 0.2536\n",
      "| end of epoch 306 | time:  0.19s | train_loss 8.3882 | valid rse 0.7725 | valid rae 0.7096 | valid corr  0.2866\n",
      "| end of epoch 307 | time:  0.24s | train_loss 8.3658 | valid rse 0.7788 | valid rae 0.7122 | valid corr  0.2770\n",
      "| end of epoch 308 | time:  0.21s | train_loss 8.3838 | valid rse 0.7666 | valid rae 0.7057 | valid corr  0.2884\n",
      "| end of epoch 309 | time:  0.18s | train_loss 8.3875 | valid rse 0.7702 | valid rae 0.7095 | valid corr  0.2927\n",
      "| end of epoch 310 | time:  0.18s | train_loss 8.3801 | valid rse 0.7796 | valid rae 0.7154 | valid corr  0.2860\n",
      "test rse 0.7687 | test rae 0.7142 | test corr 0.2612\n",
      "| end of epoch 311 | time:  0.18s | train_loss 8.3789 | valid rse 0.7764 | valid rae 0.7129 | valid corr  0.2823\n",
      "| end of epoch 312 | time:  0.19s | train_loss 8.3693 | valid rse 0.7751 | valid rae 0.7101 | valid corr  0.2837\n",
      "| end of epoch 313 | time:  0.20s | train_loss 8.3639 | valid rse 0.7775 | valid rae 0.7126 | valid corr  0.2861\n",
      "| end of epoch 314 | time:  0.24s | train_loss 8.3544 | valid rse 0.7867 | valid rae 0.7216 | valid corr  0.2821\n",
      "| end of epoch 315 | time:  0.18s | train_loss 8.3623 | valid rse 0.7697 | valid rae 0.7100 | valid corr  0.2808\n",
      "test rse 0.7669 | test rae 0.7142 | test corr 0.2480\n",
      "| end of epoch 316 | time:  0.20s | train_loss 8.3545 | valid rse 0.7700 | valid rae 0.7094 | valid corr  0.2856\n",
      "| end of epoch 317 | time:  0.20s | train_loss 8.3639 | valid rse 0.7797 | valid rae 0.7149 | valid corr  0.2875\n",
      "| end of epoch 318 | time:  0.24s | train_loss 8.3564 | valid rse 0.7775 | valid rae 0.7141 | valid corr  0.2883\n",
      "| end of epoch 319 | time:  0.18s | train_loss 8.3491 | valid rse 0.7728 | valid rae 0.7086 | valid corr  0.2841\n",
      "| end of epoch 320 | time:  0.19s | train_loss 8.3536 | valid rse 0.7703 | valid rae 0.7106 | valid corr  0.2857\n",
      "test rse 0.7737 | test rae 0.7241 | test corr 0.2456\n",
      "| end of epoch 321 | time:  0.19s | train_loss 8.3606 | valid rse 0.7752 | valid rae 0.7175 | valid corr  0.2787\n",
      "| end of epoch 322 | time:  0.19s | train_loss 8.3587 | valid rse 0.7778 | valid rae 0.7151 | valid corr  0.2832\n",
      "| end of epoch 323 | time:  0.19s | train_loss 8.3514 | valid rse 0.7825 | valid rae 0.7144 | valid corr  0.2849\n",
      "| end of epoch 324 | time:  0.19s | train_loss 8.3434 | valid rse 0.7812 | valid rae 0.7155 | valid corr  0.2816\n",
      "| end of epoch 325 | time:  0.27s | train_loss 8.3372 | valid rse 0.7736 | valid rae 0.7109 | valid corr  0.2816\n",
      "test rse 0.7690 | test rae 0.7146 | test corr 0.2500\n",
      "| end of epoch 326 | time:  0.24s | train_loss 8.3483 | valid rse 0.7726 | valid rae 0.7101 | valid corr  0.2874\n",
      "| end of epoch 327 | time:  0.18s | train_loss 8.3219 | valid rse 0.7744 | valid rae 0.7111 | valid corr  0.2789\n",
      "| end of epoch 328 | time:  0.19s | train_loss 8.3203 | valid rse 0.7730 | valid rae 0.7105 | valid corr  0.2819\n",
      "| end of epoch 329 | time:  0.18s | train_loss 8.3353 | valid rse 0.7785 | valid rae 0.7153 | valid corr  0.2788\n",
      "| end of epoch 330 | time:  0.18s | train_loss 8.3251 | valid rse 0.7798 | valid rae 0.7157 | valid corr  0.2864\n",
      "test rse 0.7709 | test rae 0.7160 | test corr 0.2547\n",
      "| end of epoch 331 | time:  0.27s | train_loss 8.3281 | valid rse 0.7739 | valid rae 0.7112 | valid corr  0.2886\n",
      "| end of epoch 332 | time:  0.24s | train_loss 8.3328 | valid rse 0.7664 | valid rae 0.7065 | valid corr  0.2844\n",
      "| end of epoch 333 | time:  0.22s | train_loss 8.3178 | valid rse 0.7663 | valid rae 0.7074 | valid corr  0.2822\n",
      "| end of epoch 334 | time:  0.19s | train_loss 8.3560 | valid rse 0.7820 | valid rae 0.7195 | valid corr  0.2770\n",
      "| end of epoch 335 | time:  0.19s | train_loss 8.3534 | valid rse 0.7774 | valid rae 0.7164 | valid corr  0.2817\n",
      "test rse 0.7741 | test rae 0.7250 | test corr 0.2623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch 336 | time:  0.18s | train_loss 8.3222 | valid rse 0.7695 | valid rae 0.7090 | valid corr  0.2818\n",
      "| end of epoch 337 | time:  0.23s | train_loss 8.3208 | valid rse 0.7681 | valid rae 0.7065 | valid corr  0.2816\n",
      "| end of epoch 338 | time:  0.19s | train_loss 8.3227 | valid rse 0.7778 | valid rae 0.7161 | valid corr  0.2819\n",
      "| end of epoch 339 | time:  0.21s | train_loss 8.3139 | valid rse 0.7729 | valid rae 0.7146 | valid corr  0.2842\n",
      "| end of epoch 340 | time:  0.19s | train_loss 8.3079 | valid rse 0.7685 | valid rae 0.7090 | valid corr  0.2841\n",
      "test rse 0.7677 | test rae 0.7142 | test corr 0.2527\n",
      "| end of epoch 341 | time:  0.19s | train_loss 8.3135 | valid rse 0.7760 | valid rae 0.7164 | valid corr  0.2755\n",
      "| end of epoch 342 | time:  0.18s | train_loss 8.2952 | valid rse 0.7681 | valid rae 0.7091 | valid corr  0.2822\n",
      "| end of epoch 343 | time:  0.18s | train_loss 8.2822 | valid rse 0.7648 | valid rae 0.7067 | valid corr  0.2853\n",
      "| end of epoch 344 | time:  0.19s | train_loss 8.2948 | valid rse 0.7661 | valid rae 0.7079 | valid corr  0.2840\n",
      "| end of epoch 345 | time:  0.23s | train_loss 8.2865 | valid rse 0.7654 | valid rae 0.7068 | valid corr  0.2830\n",
      "test rse 0.7694 | test rae 0.7199 | test corr 0.2457\n",
      "| end of epoch 346 | time:  0.19s | train_loss 8.2966 | valid rse 0.7679 | valid rae 0.7077 | valid corr  0.2858\n",
      "| end of epoch 347 | time:  0.22s | train_loss 8.2883 | valid rse 0.7780 | valid rae 0.7163 | valid corr  0.2791\n",
      "| end of epoch 348 | time:  0.18s | train_loss 8.2896 | valid rse 0.7622 | valid rae 0.7066 | valid corr  0.2838\n",
      "| end of epoch 349 | time:  0.20s | train_loss 8.2869 | valid rse 0.7627 | valid rae 0.7044 | valid corr  0.2904\n",
      "| end of epoch 350 | time:  0.19s | train_loss 8.2916 | valid rse 0.7685 | valid rae 0.7093 | valid corr  0.2851\n",
      "test rse 0.7707 | test rae 0.7182 | test corr 0.2625\n",
      "| end of epoch 351 | time:  0.24s | train_loss 8.2856 | valid rse 0.7695 | valid rae 0.7086 | valid corr  0.2853\n",
      "| end of epoch 352 | time:  0.18s | train_loss 8.2781 | valid rse 0.7729 | valid rae 0.7137 | valid corr  0.2755\n",
      "| end of epoch 353 | time:  0.18s | train_loss 8.2866 | valid rse 0.7694 | valid rae 0.7109 | valid corr  0.2846\n",
      "| end of epoch 354 | time:  0.19s | train_loss 8.2850 | valid rse 0.7723 | valid rae 0.7135 | valid corr  0.2875\n",
      "| end of epoch 355 | time:  0.19s | train_loss 8.2794 | valid rse 0.7674 | valid rae 0.7068 | valid corr  0.2884\n",
      "test rse 0.7639 | test rae 0.7119 | test corr 0.2596\n",
      "| end of epoch 356 | time:  0.19s | train_loss 8.2809 | valid rse 0.7749 | valid rae 0.7204 | valid corr  0.2743\n",
      "| end of epoch 357 | time:  0.26s | train_loss 8.2801 | valid rse 0.7839 | valid rae 0.7229 | valid corr  0.2773\n",
      "| end of epoch 358 | time:  0.19s | train_loss 8.2684 | valid rse 0.7710 | valid rae 0.7126 | valid corr  0.2805\n",
      "| end of epoch 359 | time:  0.18s | train_loss 8.2958 | valid rse 0.7625 | valid rae 0.7031 | valid corr  0.2895\n",
      "| end of epoch 360 | time:  0.18s | train_loss 8.2860 | valid rse 0.7646 | valid rae 0.7057 | valid corr  0.2883\n",
      "test rse 0.7650 | test rae 0.7132 | test corr 0.2563\n",
      "| end of epoch 361 | time:  0.18s | train_loss 8.2722 | valid rse 0.7677 | valid rae 0.7093 | valid corr  0.2850\n",
      "| end of epoch 362 | time:  0.19s | train_loss 8.2715 | valid rse 0.7653 | valid rae 0.7078 | valid corr  0.2822\n",
      "| end of epoch 363 | time:  0.18s | train_loss 8.2775 | valid rse 0.7660 | valid rae 0.7081 | valid corr  0.2834\n",
      "| end of epoch 364 | time:  0.23s | train_loss 8.2644 | valid rse 0.7732 | valid rae 0.7157 | valid corr  0.2849\n",
      "| end of epoch 365 | time:  0.18s | train_loss 8.2641 | valid rse 0.7778 | valid rae 0.7171 | valid corr  0.2907\n",
      "test rse 0.7740 | test rae 0.7242 | test corr 0.2556\n",
      "| end of epoch 366 | time:  0.18s | train_loss 8.2656 | valid rse 0.7741 | valid rae 0.7136 | valid corr  0.2857\n",
      "| end of epoch 367 | time:  0.18s | train_loss 8.2627 | valid rse 0.7689 | valid rae 0.7108 | valid corr  0.2858\n",
      "| end of epoch 368 | time:  0.24s | train_loss 8.2786 | valid rse 0.7671 | valid rae 0.7098 | valid corr  0.2850\n",
      "| end of epoch 369 | time:  0.19s | train_loss 8.2679 | valid rse 0.7776 | valid rae 0.7206 | valid corr  0.2716\n",
      "| end of epoch 370 | time:  0.21s | train_loss 8.2642 | valid rse 0.7782 | valid rae 0.7179 | valid corr  0.2704\n",
      "test rse 0.7805 | test rae 0.7307 | test corr 0.2375\n",
      "| end of epoch 371 | time:  0.19s | train_loss 8.2498 | valid rse 0.7707 | valid rae 0.7113 | valid corr  0.2855\n",
      "| end of epoch 372 | time:  0.20s | train_loss 8.2533 | valid rse 0.7725 | valid rae 0.7128 | valid corr  0.2793\n",
      "| end of epoch 373 | time:  0.18s | train_loss 8.2625 | valid rse 0.7791 | valid rae 0.7206 | valid corr  0.2777\n",
      "| end of epoch 374 | time:  0.20s | train_loss 8.2677 | valid rse 0.7718 | valid rae 0.7114 | valid corr  0.2780\n",
      "| end of epoch 375 | time:  0.22s | train_loss 8.2523 | valid rse 0.7740 | valid rae 0.7118 | valid corr  0.2834\n",
      "test rse 0.7663 | test rae 0.7151 | test corr 0.2544\n",
      "| end of epoch 376 | time:  0.19s | train_loss 8.2445 | valid rse 0.7685 | valid rae 0.7070 | valid corr  0.2910\n",
      "| end of epoch 377 | time:  0.18s | train_loss 8.2487 | valid rse 0.7743 | valid rae 0.7130 | valid corr  0.2876\n",
      "| end of epoch 378 | time:  0.19s | train_loss 8.2409 | valid rse 0.7839 | valid rae 0.7222 | valid corr  0.2796\n",
      "| end of epoch 379 | time:  0.21s | train_loss 8.2334 | valid rse 0.7770 | valid rae 0.7178 | valid corr  0.2814\n",
      "| end of epoch 380 | time:  0.23s | train_loss 8.2477 | valid rse 0.7673 | valid rae 0.7099 | valid corr  0.2791\n",
      "test rse 0.7704 | test rae 0.7214 | test corr 0.2400\n",
      "| end of epoch 381 | time:  0.18s | train_loss 8.2401 | valid rse 0.7761 | valid rae 0.7142 | valid corr  0.2796\n",
      "| end of epoch 382 | time:  0.21s | train_loss 8.2389 | valid rse 0.7887 | valid rae 0.7257 | valid corr  0.2818\n",
      "| end of epoch 383 | time:  0.21s | train_loss 8.2323 | valid rse 0.7635 | valid rae 0.7038 | valid corr  0.2866\n",
      "| end of epoch 384 | time:  0.19s | train_loss 8.2542 | valid rse 0.7626 | valid rae 0.7040 | valid corr  0.2890\n",
      "| end of epoch 385 | time:  0.19s | train_loss 8.2376 | valid rse 0.7718 | valid rae 0.7163 | valid corr  0.2839\n",
      "test rse 0.7796 | test rae 0.7343 | test corr 0.2560\n",
      "| end of epoch 386 | time:  0.20s | train_loss 8.2286 | valid rse 0.7707 | valid rae 0.7128 | valid corr  0.2904\n",
      "| end of epoch 387 | time:  0.21s | train_loss 8.2442 | valid rse 0.7723 | valid rae 0.7132 | valid corr  0.2805\n",
      "| end of epoch 388 | time:  0.20s | train_loss 8.2381 | valid rse 0.7818 | valid rae 0.7213 | valid corr  0.2743\n",
      "| end of epoch 389 | time:  0.19s | train_loss 8.2207 | valid rse 0.7781 | valid rae 0.7162 | valid corr  0.2837\n",
      "| end of epoch 390 | time:  0.21s | train_loss 8.2185 | valid rse 0.7774 | valid rae 0.7160 | valid corr  0.2836\n",
      "test rse 0.7778 | test rae 0.7275 | test corr 0.2402\n",
      "| end of epoch 391 | time:  0.22s | train_loss 8.2074 | valid rse 0.7664 | valid rae 0.7117 | valid corr  0.2829\n",
      "| end of epoch 392 | time:  0.22s | train_loss 8.2115 | valid rse 0.7675 | valid rae 0.7100 | valid corr  0.2866\n",
      "| end of epoch 393 | time:  0.23s | train_loss 8.2253 | valid rse 0.7677 | valid rae 0.7076 | valid corr  0.2905\n",
      "| end of epoch 394 | time:  0.22s | train_loss 8.2327 | valid rse 0.7669 | valid rae 0.7089 | valid corr  0.2840\n",
      "| end of epoch 395 | time:  0.22s | train_loss 8.2359 | valid rse 0.7661 | valid rae 0.7089 | valid corr  0.2878\n",
      "test rse 0.7714 | test rae 0.7237 | test corr 0.2542\n",
      "| end of epoch 396 | time:  0.19s | train_loss 8.2127 | valid rse 0.7714 | valid rae 0.7118 | valid corr  0.2824\n",
      "| end of epoch 397 | time:  0.20s | train_loss 8.2203 | valid rse 0.7973 | valid rae 0.7345 | valid corr  0.2716\n",
      "| end of epoch 398 | time:  0.18s | train_loss 8.2073 | valid rse 0.7820 | valid rae 0.7201 | valid corr  0.2788\n",
      "| end of epoch 399 | time:  0.19s | train_loss 8.2012 | valid rse 0.7646 | valid rae 0.7078 | valid corr  0.2864\n",
      "| end of epoch 400 | time:  0.19s | train_loss 8.2095 | valid rse 0.7674 | valid rae 0.7097 | valid corr  0.2917\n",
      "test rse 0.7746 | test rae 0.7248 | test corr 0.2493\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''--gpu 3 \n",
    "   --data data/traffic.txt \n",
    "   --save save/traffic.pt \n",
    "   --hidSkip 10'''\n",
    "\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    print('begin training');\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train(Data, Data.train[0], Data.train[1], model, criterion, optim, args.batch_size)\n",
    "        val_loss, val_rae, val_corr, _, _ = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | train_loss {:5.4f} | valid rse {:5.4f} | valid rae {:5.4f} | valid corr  {:5.4f}'.format(epoch, (time.time() - epoch_start_time), train_loss, val_loss, val_rae, val_corr))\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val = val_loss\n",
    "        if epoch % 5 == 0:\n",
    "            test_acc, test_rae, test_corr, _, _  = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "            print (\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec207a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rse 0.7660 | test rae 0.7176 | test corr 0.2515\n",
      "(389, 367)\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "test_acc, test_rae, test_corr, predict, Ytest  = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1, args.batch_size);\n",
    "print (\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
    "\n",
    "print(Ytest.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a83b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1 = []\n",
    "\n",
    "ln = 750\n",
    "i = 2\n",
    "\n",
    "xl = [i for i in range(ln)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8033fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5528a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  plt.figure(figsize=(15,8))\n",
    "  plt.plot(Ytest[:ln,i], label = \"Y-TEST\")\n",
    "  plt.plot(predict[:ln,i], label = \"Prediction\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
